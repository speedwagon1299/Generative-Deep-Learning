{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from keras.layers import Dense, Conv2D, Conv2DTranspose, Flatten, BatchNormalization, Input, Reshape\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import random\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a random seed\n",
    "np.random.seed(51)\n",
    "\n",
    "# parameters for building the model and training\n",
    "BATCH_SIZE=2000\n",
    "LATENT_DIM=512\n",
    "IMAGE_SIZE=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the data directory\n",
    "try:\n",
    "  os.mkdir('/tmp/anime')\n",
    "except OSError:\n",
    "  pass\n",
    "\n",
    "# download the zipped dataset to the data directory\n",
    "data_url = \"https://storage.googleapis.com/learning-datasets/Resources/anime-faces.zip\"\n",
    "data_file_name = \"animefaces.zip\"\n",
    "download_dir = '/tmp/anime/'\n",
    "urllib.request.urlretrieve(data_url, data_file_name)\n",
    "\n",
    "# extract the zip file\n",
    "zip_ref = zipfile.ZipFile(data_file_name, 'r')\n",
    "zip_ref.extractall(download_dir)\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation Utilities\n",
    "\n",
    "def get_dataset_slice_paths(image_dir):\n",
    "  '''returns a list of paths to the image files'''\n",
    "  image_file_list = os.listdir(image_dir)\n",
    "  image_paths = [os.path.join(image_dir, fname) for fname in image_file_list]\n",
    "\n",
    "  return image_paths\n",
    "\n",
    "\n",
    "def map_image(image_filename):\n",
    "  '''preprocesses the images'''\n",
    "  img_raw = tf.io.read_file(image_filename)\n",
    "  image = tf.image.decode_jpeg(img_raw)\n",
    "\n",
    "  image = tf.cast(image, dtype=tf.float32)\n",
    "  image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "  image = image / 255.0  \n",
    "  image = tf.reshape(image, shape=(IMAGE_SIZE, IMAGE_SIZE, 3,))\n",
    "\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list containing the image paths\n",
    "paths = get_dataset_slice_paths(\"/tmp/anime/images/\")\n",
    "\n",
    "# shuffle the paths\n",
    "random.shuffle(paths)\n",
    "\n",
    "# split the paths list into to training (80%) and validation sets(20%).\n",
    "paths_len = len(paths)\n",
    "train_paths_len = int(paths_len * 0.8)\n",
    "\n",
    "train_paths = paths[:train_paths_len]\n",
    "val_paths = paths[train_paths_len:]\n",
    "\n",
    "# load the training image paths into tensors, create batches and shuffle\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices((train_paths))\n",
    "training_dataset = training_dataset.map(map_image)\n",
    "training_dataset = training_dataset.shuffle(1000).batch(BATCH_SIZE)\n",
    "\n",
    "# load the validation image paths into tensors and create batches\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((val_paths))\n",
    "validation_dataset = validation_dataset.map(map_image)\n",
    "validation_dataset = validation_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "print(f'number of batches in the training set: {len(training_dataset)}')\n",
    "print(f'number of batches in the validation set: {len(validation_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_faces(dataset, size=9):\n",
    "  '''Takes a sample from a dataset batch and plots it in a grid.'''\n",
    "  dataset = dataset.unbatch().take(size)\n",
    "  n_cols = 3\n",
    "  n_rows = size//n_cols + 1\n",
    "  plt.figure(figsize=(5, 5))\n",
    "  i = 0\n",
    "  for image in dataset:\n",
    "    i += 1\n",
    "    disp_img = np.reshape(image, (64,64,3))\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(disp_img)\n",
    "\n",
    "\n",
    "def display_one_row(disp_images, offset, shape=(28, 28)):\n",
    "  '''Displays a row of images.'''\n",
    "  for idx, image in enumerate(disp_images):\n",
    "    plt.subplot(3, 10, offset + idx + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    image = np.reshape(image, shape)\n",
    "    plt.imshow(image)\n",
    "\n",
    "\n",
    "def display_results(disp_input_images, disp_predicted):\n",
    "  '''Displays input and predicted images.'''\n",
    "  plt.figure(figsize=(15, 5))\n",
    "  display_one_row(disp_input_images, 0, shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n",
    "  display_one_row(disp_predicted, 20, shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_faces(validation_dataset, size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(tf.keras.layers.Layer):\n",
    "  def call(self, inputs):\n",
    "    \"\"\"Generates a random sample and combines with the encoder output\n",
    "    \n",
    "    Args:\n",
    "      inputs -- output tensor from the encoder\n",
    "\n",
    "    Returns:\n",
    "      `inputs` tensors combined with a random sample\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    mu, sigma = inputs\n",
    "    batch = tf.shape(sigma)[0]\n",
    "    dim = tf.shape(sigma)[1]\n",
    "    epsilon = tf.keras.backend.random_normal(shape = (batch,dim))\n",
    "    z = mu + tf.math.exp(0.5*sigma)*epsilon\n",
    "    ### END CODE HERE ###\n",
    "    return  z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layers(inputs, latent_dim):\n",
    "  \"\"\"Defines the encoder's layers.\n",
    "  Args:\n",
    "    inputs -- batch from the dataset\n",
    "    latent_dim -- dimensionality of the latent space\n",
    "\n",
    "  Returns:\n",
    "    mu -- learned mean\n",
    "    sigma -- learned standard deviation\n",
    "    batch_3.shape -- shape of the features before flattening\n",
    "  \"\"\"\n",
    "  ### START CODE HERE ###\n",
    "  x = Conv2D(filters = 32, padding = 'same', kernel_size = (3,3), strides = 2, activation = 'relu', name = 'enc_conv_1') (inputs)\n",
    "  x = BatchNormalization() (x)\n",
    "  x = Conv2D(filters = 64, padding = 'same', kernel_size = (3,3), strides = 2, activation = 'relu', name = 'enc_conv_2') (x)\n",
    "  x = BatchNormalization() (x)\n",
    "  x = Conv2D(filters = 128, padding = 'same', kernel_size = (3,3), strides = 2, activation = 'relu', name = 'enc_conv_3') (x)\n",
    "  batch_3 = BatchNormalization() (x)\n",
    "  x = Flatten(name = 'enc_flatten') (batch_3)\n",
    "  x = Dense(units = 512, activation = 'relu', name = 'enc_dense') (x)\n",
    "  x = BatchNormalization() (x)\n",
    "  mu = Dense(latent_dim, activation = 'relu', name = 'enc_mu') (x)\n",
    "  sigma = Dense(latent_dim, activation = 'relu', name = 'enc_sigma')  (x)\n",
    "  \n",
    "  ### END CODE HERE ###\n",
    "\n",
    "  # revise `batch_3.shape` here if you opted not to use 3 Conv2D layers\n",
    "  return mu, sigma, batch_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_model(latent_dim, input_shape):\n",
    "  \"\"\"Defines the encoder model with the Sampling layer\n",
    "  Args:\n",
    "    latent_dim -- dimensionality of the latent space\n",
    "    input_shape -- shape of the dataset batch\n",
    "\n",
    "  Returns:\n",
    "    model -- the encoder model\n",
    "    conv_shape -- shape of the features before flattening\n",
    "  \"\"\"\n",
    "  ### START CODE HERE ###\n",
    "  inputs = Input(shape = input_shape)\n",
    "  mu, sigma, conv_shape = encoder_layers(inputs, latent_dim)\n",
    "  z = Sampling()((mu,sigma))\n",
    "  model = tf.keras.Model(inputs = inputs, outputs = [mu,sigma,z])\n",
    "  ### END CODE HERE ###\n",
    "  model.summary()\n",
    "  return model, conv_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layers(inputs, conv_shape):\n",
    "  \"\"\"Defines the decoder layers.\n",
    "  Args:\n",
    "    inputs -- output of the encoder \n",
    "    conv_shape -- shape of the features before flattening\n",
    "\n",
    "  Returns:\n",
    "    tensor containing the decoded output\n",
    "  \"\"\"\n",
    "  ### START CODE HERE ###\n",
    "  shape = conv_shape[1] * conv_shape[2] * conv_shape[3] \n",
    "  x = Dense(units = shape, activation = 'relu', name = 'dec_dense') (inputs)\n",
    "  x = BatchNormalization() (x)\n",
    "  x = Reshape((conv_shape[1],conv_shape[2],conv_shape[3]), name = 'dec_reshape') (x)\n",
    "  x = Conv2DTranspose(128, kernel_size = (3,3), strides = 2, padding = 'same', activation = 'relu', name = 'dec_conv_1') (x)\n",
    "  x = BatchNormalization() (x)\n",
    "  x = Conv2DTranspose(64, kernel_size = (3,3), strides = 2, padding = 'same', activation = 'relu', name = 'dec_conv_2') (x)\n",
    "  x = BatchNormalization() (x)\n",
    "  x = Conv2DTranspose(32, kernel_size = (3,3), strides = 2, padding = 'same', activation = 'relu', name = 'dec_conv_3') (x)\n",
    "  x = BatchNormalization() (x)\n",
    "  x = Conv2DTranspose(3, kernel_size = (3,3), strides = 1, padding = 'same', activation = 'sigmoid', name = 'dec_conv_final') (x)\n",
    "  \n",
    "  ### END CODE HERE ###\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_model(latent_dim, conv_shape):\n",
    "  \"\"\"Defines the decoder model.\n",
    "  Args:\n",
    "    latent_dim -- dimensionality of the latent space\n",
    "    conv_shape -- shape of the features before flattening\n",
    "\n",
    "  Returns:\n",
    "    model -- the decoder model\n",
    "  \"\"\"\n",
    "  ### START CODE HERE ###\n",
    "  inputs = Input(shape = (latent_dim,))\n",
    "  outputs = decoder_layers(inputs, conv_shape)\n",
    "  model = tf.keras.Model(inputs, outputs)\n",
    "  ### END CODE HERE ###\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_reconstruction_loss(mu, sigma):\n",
    "  \"\"\" Computes the Kullback-Leibler Divergence (KLD)\n",
    "  Args:\n",
    "    mu -- mean\n",
    "    sigma -- standard deviation\n",
    "\n",
    "  Returns:\n",
    "    KLD loss\n",
    "  \"\"\"\n",
    "  kl_loss = 1 + sigma - tf.square(mu) - tf.math.exp(sigma)\n",
    "  return tf.reduce_mean(kl_loss) * -0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_model(encoder, decoder, input_shape):\n",
    "  \"\"\"Defines the VAE model\n",
    "  Args:\n",
    "    encoder -- the encoder model\n",
    "    decoder -- the decoder model\n",
    "    input_shape -- shape of the dataset batch\n",
    "\n",
    "  Returns:\n",
    "    the complete VAE model\n",
    "  \"\"\"\n",
    "  ### START CODE HERE ###\n",
    "  inputs = Input(shape = input_shape) \n",
    "  mu = encoder(inputs)[0]\n",
    "  sigma = encoder(inputs)[1]\n",
    "  z = encoder(inputs)[2]\n",
    "  recon = decoder(z)\n",
    "  model = tf.keras.Model(inputs = inputs, outputs = recon)\n",
    "  kl_loss = kl_reconstruction_loss(mu, sigma)\n",
    "  model.add_loss(kl_loss)\n",
    "  \n",
    "  ### END CODE HERE ###\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(input_shape, latent_dim):\n",
    "  \"\"\"Returns the encoder, decoder, and vae models\"\"\"\n",
    "  ### START CODE HERE ###\n",
    "  encoder, conv_shape = encoder_model(latent_dim,input_shape)\n",
    "  decoder = decoder_model(latent_dim,conv_shape)\n",
    "  vae = vae_model(encoder, decoder, input_shape)\n",
    "  ### END CODE HERE ###\n",
    "  return encoder, decoder, vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder, vae = get_models(input_shape=(64,64,3,), latent_dim=LATENT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "mse_loss = tf.keras.losses.MeanSquaredError()\n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, step, test_input):\n",
    "  \"\"\"Helper function to plot our 16 images\n",
    "\n",
    "  Args:\n",
    "\n",
    "  model -- the decoder model\n",
    "  epoch -- current epoch number during training\n",
    "  step -- current step number during training\n",
    "  test_input -- random tensor with shape (16, LATENT_DIM)\n",
    "  \"\"\"\n",
    "  predictions = model.predict(test_input)\n",
    "\n",
    "  fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      img = predictions[i, :, :, :] * 255\n",
    "      img = img.astype('int32')\n",
    "      plt.imshow(img)\n",
    "      plt.axis('off')\n",
    "\n",
    "  # tight_layout minimizes the overlap between 2 sub-plots\n",
    "  fig.suptitle(\"epoch: {}, step: {}\".format(epoch, step))\n",
    "  plt.savefig('image_at_epoch_{:04d}_step{:04d}.png'.format(epoch, step))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop. Display generated images each epoch\n",
    "\n",
    "### START CODE HERE ###\n",
    "epochs = 40\n",
    "### END CODE HERE ###\n",
    "\n",
    "random_vector_for_generation = tf.random.normal(shape=[16, LATENT_DIM])\n",
    "generate_and_save_images(decoder, 0, 0, random_vector_for_generation)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "  # Iterate over the batches of the dataset.\n",
    "  for step, x_batch_train in enumerate(training_dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "      ### START CODE HERE ### \n",
    "      recon = vae(x_batch_train)\n",
    "      flattened_inputs = tf.reshape(x_batch_train, shape = [-1])\n",
    "      flattened_outputs = tf.reshape(recon, shape = [-1])\n",
    "      loss = mse_loss(flattened_inputs, flattened_outputs)*64*64*3\n",
    "      loss += sum(vae.losses)\n",
    "    grads = tape.gradient(loss,vae.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads,vae.trainable_weights))\n",
    "    ### END CODE HERE\n",
    "    \n",
    "    loss_metric(loss)\n",
    "\n",
    "    if step % 10 == 0:\n",
    "      display.clear_output(wait=False)    \n",
    "      generate_and_save_images(decoder, epoch, step, random_vector_for_generation)\n",
    "    print('Epoch: %s step: %s mean loss = %s' % (epoch, step, loss_metric.result().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = validation_dataset.take(1)\n",
    "output_samples = []\n",
    "\n",
    "for input_image in tfds.as_numpy(test_dataset):\n",
    "      output_samples = input_image\n",
    "\n",
    "idxs = np.random.choice(64, size=10)\n",
    "\n",
    "vae_predicted = vae.predict(test_dataset)\n",
    "display_results(output_samples[idxs], vae_predicted[idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(rows, cols, images, title):\n",
    "    '''Displays images in a grid.'''\n",
    "    grid = np.zeros(shape=(rows*64, cols*64, 3))\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            grid[row*64:(row+1)*64, col*64:(col+1)*64, :] = images[row*cols + col]\n",
    "\n",
    "    plt.figure(figsize=(12,12))       \n",
    "    plt.imshow(grid)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# initialize random inputs\n",
    "test_vector_for_generation = tf.random.normal(shape=[64, LATENT_DIM])\n",
    "\n",
    "# get predictions from the decoder model\n",
    "predictions= decoder.predict(test_vector_for_generation)\n",
    "\n",
    "# plot the predictions\n",
    "plot_images(8,8,predictions,'Generated Images')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
